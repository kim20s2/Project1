{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization Tutorial\n",
    "\n",
    "This tutorial describe the process of optimizing the user's model. The input to this tutorial is a HAR file in Hailo Model state (before optimization; with native weights) and the output will be a quantized HAR file with quantized weights.\n",
    "\n",
    "Note: For full information about Optimization and Quantization, refer to the `Dataflow Compiler user guide / Model optimization` section.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* Run this code in Jupyter notebook. See the Introduction tutorial for more details.\n",
    "* The user should review the complete Parsing Tutorial (or created the HAR file in other way)\n",
    "\n",
    "**Recommendation:**\n",
    "\n",
    "* To obtain best performance run this code with a GPU machine. For full information see the `Dataflow Compiler user guide / Model optimization` section.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "* Quick optimization tutorial\n",
    "* In-depth optimization & evaluation tutorial\n",
    "* Advanced Model Modifications tutorial\n",
    "* Compression and Optimization levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports used throughout the tutorial\n",
    "# file operations\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import SVG\n",
    "from matplotlib import patches\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.python.eager.context import eager_mode\n",
    "\n",
    "# import the hailo sdk client relevant classes\n",
    "from hailo_sdk_client import ClientRunner, InferenceContext\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "IMAGES_TO_VISUALIZE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Optimization Tutorial\n",
    "\n",
    "After the HAR file has been created (using either `runner.translate_tf_model` or `runner.translate_onnx_model`), the next step is to go through the optimization process.\n",
    "\n",
    "The basic optimization is performed just by calling `runner.optimize(calib_dataset)` (or the CLI `hailo optimize` command), as described on the user guide on: Building Models / Model optimization / Model Optimization Workflow.\n",
    "The calibration dataset should be preprocessed according to the model's input requirements and it is recommended to have at least 1024 inputs and to use a GPU.\n",
    "During this step it is also possible to use a model script which change the default behavior of the Dataflow Compiler, for example, to add additional layer for normalization.\n",
    "All the model script available commands are described in the user guide on: Building Models / Model optimization / Optimization Related Model Script Commands.\n",
    "\n",
    "In order to learn how to deal with common pitfalls, image formats and accuracy, refer to the in-depth section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will prepare the calibration set. Resize the images to the correct size and crop them.\n",
    "def preproc(image, output_height=224, output_width=224, resize_side=256):\n",
    "    \"\"\"imagenet-standard: aspect-preserving resize to 256px smaller-side, then central-crop to 224px\"\"\"\n",
    "    with eager_mode():\n",
    "        h, w = image.shape[0], image.shape[1]\n",
    "        scale = tf.cond(tf.less(h, w), lambda: resize_side / h, lambda: resize_side / w)\n",
    "        resized_image = tf.image.resize(tf.expand_dims(image, 0), [int(h * scale), int(w * scale)])\n",
    "        cropped_image = tf.image.resize_with_crop_or_pad(resized_image, output_height, output_width)\n",
    "\n",
    "        return tf.squeeze(cropped_image)\n",
    "\n",
    "\n",
    "images_path = \"../data\"\n",
    "images_list = [img_name for img_name in os.listdir(images_path) if os.path.splitext(img_name)[1] == \".jpg\"]\n",
    "\n",
    "calib_dataset = np.zeros((len(images_list), 224, 224, 3))\n",
    "for idx, img_name in enumerate(sorted(images_list)):\n",
    "    img = np.array(Image.open(os.path.join(images_path, img_name)))\n",
    "    img_preproc = preproc(img)\n",
    "    calib_dataset[idx, :, :, :] = img_preproc.numpy()\n",
    "\n",
    "np.save(\"calib_set.npy\", calib_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, we will load our parsed HAR from the Parsing Tutorial\n",
    "\n",
    "model_name = \"resnet_v1_18\"\n",
    "hailo_model_har_name = f\"{model_name}_hailo_model.har\"\n",
    "assert os.path.isfile(hailo_model_har_name), \"Please provide valid path for HAR file\"\n",
    "runner = ClientRunner(har=hailo_model_har_name)\n",
    "# By default it uses the hw_arch that is saved on the HAR. For overriding, use the hw_arch flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will create a model script, that tells the compiler to add a normalization on the beginning\n",
    "# of the model (that is why we didn't normalize the calibration set;\n",
    "# Otherwise we would have to normalize it before using it)\n",
    "\n",
    "# Batch size is 8 by default\n",
    "alls = \"normalization1 = normalization([123.675, 116.28, 103.53], [58.395, 57.12, 57.375])\\n\"\n",
    "\n",
    "# Load the model script to ClientRunner so it will be considered on optimization\n",
    "runner.load_model_script(alls)\n",
    "\n",
    "# Call Optimize to perform the optimization process\n",
    "runner.optimize(calib_dataset)\n",
    "\n",
    "# Save the result state to a Quantized HAR file\n",
    "quantized_model_har_path = f\"{model_name}_quantized_model.har\"\n",
    "runner.save_har(quantized_model_har_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes the quick tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Depth Optimization Tutorial\n",
    "\n",
    "The advanced optimization process (see the diagram in the user guide on: Building Models / Model optimization / Model Optimization Workflow), is comprised of the following steps:\n",
    "\n",
    "1. Test the parsed `Native model` before any changes are made (still on floating point precision), check to see that the pre and post processing code works well with the start and end nodes provided. The Native model will match the results of the original model, in between the start_node_names and the end_node_names provided by the user during the Parsing stage.\n",
    "2. Optional: Apply Model Modifications (like input Normalization layer, YUY2 to RGB conversion, changing output activations and others), using a `model script`.\n",
    "3. Test the `FP Optimized model` (the model after floating point operations and modifications) to see that required results have been achieved.\n",
    "    - Note: Remember to update the pre and post processing code to match the changes in the model. For example, if normalization has been added to the model, remove the normalization code from the pre-processing code, and feed un-normalized images to the model. If softmax has been added onto the outputs, remove the softmax from the post-processing code. Etc.\n",
    "4. Now perform `Optimization` to the model, using a calibration set that has been prepared. The result is a `Quantized model`, that has some degradation compared to the pre-quantized model.\n",
    "    - Note: The format of calibration set is the same as was used as inputs for the modified model. For example, if a normalization layer has been added to the model, the calibration set should not be normalized. If this layer has not been added yet, pre-process and normalize the images.\n",
    "5. Test the quantized model using the same already-validated code for the pre and post processing.\n",
    "    - If there is a degradation, it is due to the quantization process and not due to input/output formats, as they were already verified with the pre-quantized model.\n",
    "6. To increase the accuracy of the quantized model, it is possible to optimize again using a `model script` to affect the optimization process.\n",
    "    - Note: The most basic method is to raise the optimization_level, an example model script command is `model_optimization_flavor(optimization_level=4)`. The advanced method is to use the Layer Analysis Tool, presented on the next tutorial.\n",
    "    - Note: If the accuracy is good, consider increasing the performance by using 4-bit weights. This is done using compression_level, an example model script command is `model_optimization_flavor(compression_level=2)`.\n",
    "7. During the next tutorials, compilation and on-device inference, input and output values are expected to be similar to the quantized model's values.\n",
    "\n",
    "The testing (whether on Native, Modified or Quantized model) is performed using our `Emulator` feature, that will be described in this tutorial.\n",
    "\n",
    "To further understand the advanced optimization process, the following steps are described below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary step: Create testing environment\n",
    "\n",
    "Hailo offers an `Emulator` for testing the model in its different states.  \n",
    "The emulator is implemented as a Tensorflow graph, and its results are the return value of `runner.infer(context, network_input)`.  \n",
    "To get inference results, run this API within the context manager `runner.infer_context(inference_context)` where the inference context is one of: [`InferenceContext.SDK_NATIVE`, `InferenceContext.SDK_FP_OPTIMIZED`, `InferenceContext.SDK_QUANTIZED`]:  \n",
    "\n",
    "* `InferenceContext.SDK_NATIVE`: Testing method for Step 1 of the optimization process steps (`Native model`). Runs the model as is without any changes. Use it to make sure the model has been converted properly into Hailo's internal representation. Should yield exact results as the original model.\n",
    "* `InferenceContext.SDK_FP_OPTIMIZED`: Testing method for Step 3 of the optimization process steps (`Modified model`). The modified model represents the Hailo model prior to quantization, and is the result of performing model modifications (e.g. normalizing/resizing inputs) and full precision optimizations (e.g. tiled squeeze & excite, equalization). As a result, inference results may vary slightly from the native results.\n",
    "* `InferenceContext.SDK_QUANTIZED`: Testing method for Step 5 of the optimization process steps (`Quantized model`). This inference context emulates the hardware implementation, and is useful for measuring the overall accuracy and degradation of the quantized model. This measurement is performed against the original model over large datasets, prior to running inference on the actual Hailo device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "### Preliminary Step: Create Pre and Post Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Pre processing (prepare the input images)\n",
    "# -----------------------------------------\n",
    "def preproc(image, output_height=224, output_width=224, resize_side=256, normalize=False):\n",
    "    \"\"\"imagenet-standard: aspect-preserving resize to 256px smaller-side, then central-crop to 224px\"\"\"\n",
    "    with eager_mode():\n",
    "        h, w = image.shape[0], image.shape[1]\n",
    "        scale = tf.cond(tf.less(h, w), lambda: resize_side / h, lambda: resize_side / w)\n",
    "        resized_image = tf.image.resize(tf.expand_dims(image, 0), [int(h * scale), int(w * scale)])\n",
    "        cropped_image = tf.image.resize_with_crop_or_pad(resized_image, output_height, output_width)\n",
    "\n",
    "        if normalize:\n",
    "            # Default normalization parameters for ImageNet\n",
    "            cropped_image = (cropped_image - [123.675, 116.28, 103.53]) / [58.395, 57.12, 57.375]\n",
    "\n",
    "        return tf.squeeze(cropped_image)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Post processing (what to do with the model's outputs)\n",
    "# -----------------------------------------------------\n",
    "def _get_imagenet_labels(json_path=\"../data/imagenet_names.json\"):\n",
    "    imagenet_names = json.load(open(json_path))\n",
    "    imagenet_names = [imagenet_names[str(i)] for i in range(1001)]\n",
    "    return imagenet_names[1:]\n",
    "\n",
    "\n",
    "imagenet_labels = _get_imagenet_labels()\n",
    "\n",
    "\n",
    "def postproc(results):\n",
    "    labels = []\n",
    "    scores = []\n",
    "    results = [np.squeeze(result) for result in results]\n",
    "    for result in results:\n",
    "        top_ind = np.argmax(result)\n",
    "        cur_label = imagenet_labels[top_ind]\n",
    "        cur_score = 100 * result[top_ind]\n",
    "        labels.append(cur_label)\n",
    "        scores.append(cur_score)\n",
    "    return scores, labels\n",
    "\n",
    "\n",
    "# -------------\n",
    "# Visualization\n",
    "# -------------\n",
    "def mynorm(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "\n",
    "def visualize_results(\n",
    "    images,\n",
    "    first_scores,\n",
    "    first_labels,\n",
    "    second_scores=None,\n",
    "    second_labels=None,\n",
    "    first_title=\"Full Precision\",\n",
    "    second_title=\"Other\",\n",
    "):\n",
    "    # Deal with input arguments\n",
    "    assert (second_scores is None and second_labels is None) or (\n",
    "        second_scores is not None and second_labels is not None\n",
    "    ), \"second_scores and second_labels must both be supplied, or both not be supplied\"\n",
    "    assert len(images) == len(first_scores) == len(first_labels), \"lengths of inputs must be equal\"\n",
    "\n",
    "    show_only_first = second_scores is None\n",
    "    if not show_only_first:\n",
    "        assert len(images) == len(second_scores) == len(second_labels), \"lengths of inputs must be equal\"\n",
    "\n",
    "    # Display\n",
    "    for img_idx in range(len(images)):\n",
    "        plt.figure()\n",
    "        plt.imshow(mynorm(images[img_idx]))\n",
    "\n",
    "        if not show_only_first:\n",
    "            plt.title(\n",
    "                f\"{first_title}: top-1 class is {first_labels[img_idx]}. Confidence is {first_scores[img_idx]:.2f}%,\\n\"\n",
    "                f\"{second_title}: top-1 class is {second_labels[img_idx]}. Confidence is {second_scores[img_idx]:.2f}%\",\n",
    "            )\n",
    "        else:\n",
    "            plt.title(\n",
    "                f\"{first_title}: top-1 class is {first_labels[img_idx]}. Confidence is {first_scores[img_idx]:.2f}%\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "### Step 1: Test Native Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the network to the ClientRunner from the saved Hailo Archive file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet_v1_18\"\n",
    "hailo_model_har_name = f\"{model_name}_hailo_model.har\"\n",
    "assert os.path.isfile(hailo_model_har_name), \"Please provide valid path for HAR file\"\n",
    "runner = ClientRunner(har=hailo_model_har_name)\n",
    "# By default it uses the hw_arch that is saved on the HAR. For overriding, use the hw_arch flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = \"../data\"\n",
    "images_list = [img_name for img_name in os.listdir(images_path) if os.path.splitext(img_name)[1] == \".jpg\"]\n",
    "\n",
    "# Create an un-normalized dataset for visualization\n",
    "image_dataset = np.zeros((len(images_list), 224, 224, 3))\n",
    "# Create a normalized dataset to feed into the Native emulator\n",
    "image_dataset_normalized = np.zeros((len(images_list), 224, 224, 3))\n",
    "for idx, img_name in enumerate(sorted(images_list)):\n",
    "    img = np.array(Image.open(os.path.join(images_path, img_name)))\n",
    "    img_preproc = preproc(img)\n",
    "    image_dataset[idx, :, :, :] = img_preproc.numpy()\n",
    "    img_preproc_norm = preproc(img, normalize=True)\n",
    "    image_dataset_normalized[idx, :, :, :] = img_preproc_norm.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the `Native` emulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we use the normalized images, because normalization is not in the model\n",
    "with runner.infer_context(InferenceContext.SDK_NATIVE) as ctx:\n",
    "    native_res = runner.infer(ctx, image_dataset_normalized[:IMAGES_TO_VISUALIZE, :, :, :])\n",
    "\n",
    "native_scores, native_labels = postproc(native_res)\n",
    "visualize_results(image_dataset[:IMAGES_TO_VISUALIZE, :, :, :], native_scores, native_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2,3: Apply Model Modifications, and Test Modified Model\n",
    "\n",
    "The `Model Script` is a text file that includes `model script commands`, affecting the stages of the compiler.\n",
    "\n",
    "In the next steps the following will be performed: \n",
    "\n",
    "* Create a model script for the Optimization process, that also includes the model modifications.\n",
    "* Load the model script (it wont be applied yet)\n",
    "* Call runner.optimize_full_precision() to apply the model modifications (instead, we could call optimize() that also applies the model modifications)\n",
    "* Then the SDK_FP_OPTIMIZED emulation context can be called\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_script_lines = [\n",
    "    # Add normalization layer with mean [123.675, 116.28, 103.53] and std [58.395, 57.12, 57.375])\n",
    "    \"normalization1 = normalization([123.675, 116.28, 103.53], [58.395, 57.12, 57.375])\\n\",\n",
    "    # For multiple input nodes:\n",
    "    # {normalization_layer_name_1} = normalization([list of means per channel], [list of stds per channel], {input_layer_name_1_from_hn})\\n',\n",
    "    # {normalization_layer_name_2} = normalization([list of means per channel], [list of stds per channel], {input_layer_name_2_from_hn})\\n',\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# Load the model script to ClientRunner so it will be considered on optimization\n",
    "runner.load_model_script(\"\".join(model_script_lines))\n",
    "runner.optimize_full_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we use the original images, because normalization is IN the model\n",
    "with runner.infer_context(InferenceContext.SDK_FP_OPTIMIZED) as ctx:\n",
    "    modified_res = runner.infer(ctx, image_dataset[:IMAGES_TO_VISUALIZE, :, :, :])\n",
    "\n",
    "modified_scores, modified_labels = postproc(modified_res)\n",
    "\n",
    "visualize_results(\n",
    "    image_dataset[:IMAGES_TO_VISUALIZE, :, :, :],\n",
    "    native_scores,\n",
    "    native_labels,\n",
    "    modified_scores,\n",
    "    modified_labels,\n",
    "    second_title=\"FP Modified\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4,5: Optimize the Model and Test its Accuracy\n",
    "\n",
    "1. Create a calibration dataset (will be the same as the input to the modified model)\n",
    "2. Run Optimize\n",
    "3. Test the optimized model accuracy vs. the modified model.   \n",
    "Please note that the quantized emulator is not bit-exact with the Hailo hardware but provides good and fast approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original images are being used, just as the input to the SDK_FP_OPTIMIZED emulator\n",
    "calib_dataset = image_dataset\n",
    "\n",
    "# For calling Optimize, use the short version: runner.optimize(calib_dataset)\n",
    "# A more general approach is being used here that works also with multiple input nodes.\n",
    "# The calibration dataset may also be provided as a dictionary in the following format:\n",
    "# {input_layer_name_1_from_hn: layer_1_calib_dataset, input_layer_name_2_from_hn: layer_2_calib_dataset}\n",
    "hn_layers = runner.get_hn_dict()[\"layers\"]\n",
    "print(\"Input layers are: \")\n",
    "print([layer for layer in hn_layers if hn_layers[layer][\"type\"] == \"input_layer\"])  # See available input layer names\n",
    "calib_dataset_dict = {\"resnet_v1_18/input_layer1\": calib_dataset}  # In our case there is only one input layer\n",
    "runner.optimize(calib_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the original images are used, because normalization is present in the model\n",
    "with runner.infer_context(InferenceContext.SDK_QUANTIZED) as ctx:\n",
    "    quantized_res = runner.infer(ctx, image_dataset[:IMAGES_TO_VISUALIZE, :, :, :])\n",
    "\n",
    "quantized_scores, quantized_labels = postproc(quantized_res)\n",
    "\n",
    "visualize_results(\n",
    "    image_dataset[:IMAGES_TO_VISUALIZE, :, :, :],\n",
    "    modified_scores,\n",
    "    modified_labels,\n",
    "    quantized_scores,\n",
    "    quantized_labels,\n",
    "    first_title=\"FP Modified\",\n",
    "    second_title=\"Quantized\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the runner's state to a Quantized HAR\n",
    "quantized_model_har_path = f\"{model_name}_quantized_model.har\"\n",
    "runner.save_har(quantized_model_har_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Gpu Examples of inference\n",
    "*This Demo depends on multiple gpu availability*\n",
    "Further information for utilizing multiple GPUs is available on the `Dataflow Compiler user guide / Model optimization` section\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = len(tf.config.list_physical_devices(\"GPU\"))\n",
    "if num_gpus > 1:\n",
    "    with runner.infer_context(InferenceContext.SDK_NATIVE, gpu_policy=\"model_parallelization\") as ctx:\n",
    "        native_res = runner.infer(ctx, image_dataset_normalized)\n",
    "\n",
    "    with runner.infer_context(InferenceContext.SDK_QUANTIZED, gpu_policy=\"data_parallelization\") as ctx:\n",
    "        native_res = runner.infer(ctx, image_dataset_normalized)\n",
    "else:\n",
    "    print(f\"To run this cell at least two gpus are needed, there are only {num_gpus} available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: How to Raise Accuracy \n",
    "To increase the accuracy of the quantized model, optimize again using a model script to affect the optimization process.\n",
    "\n",
    "There are several tools that can be used.\n",
    "\n",
    "* Verify that there is a GPU with at least 1024 images in the calibration set\n",
    "* Raise the optimization_level value using the model_optimization_flavor command. If it fails on high GPU memory, try lowering the batch_size as described on the last example\n",
    "* Decrease the compression_level value using the model_optimization_flavor command (default is 0, lowest option)\n",
    "* Set the output layer(s) to use 16-bit accuracy using the command quantization_param(output_layer_name, precision_mode=a16_w16). Note that the DFC will set 16-bit output automatically for small enough outputs.\n",
    "* Use the Layer Noise Analysis tools to find layers with low SNR, and affect their quantization using weight or activation clipping (see the next tutorial)\n",
    "* Experiment with the FineTune parameters (refer to the user guide for more details)\n",
    "\n",
    "For more information refer the user guide in: Building Models / Model optimization / Model Optimization Workflow / Debugging accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the in-depth optimization tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model Modifications Tutorial\n",
    "### Adding on-chip input format conversion through model script commands \n",
    "The next cell will apply model modification commands using a model script. A [YUY2](https://en.wikipedia.org/wiki/YUV)-> [YUV](https://en.wikipedia.org/wiki/YUV)-> RGB conversion will be added.\n",
    "\n",
    "Unlike the normalization layer, which could simulate with the SDK_FP_OPTIMIZED and SDK_QUANTIZED emulators, not all format conversions are supported in the emulator (for more information see the `Dataflow Compiler user guide / Model optimization` section). \n",
    "Every conversion that runs in the emulator affects the calibration set, and the user should supply the set accordingly. \n",
    "For example, after adding YUV -> RGB format conversion layer, the calibration set is expected to be in YUV format. \n",
    "However, for some conversions the user may choose to skip the conversion in emulation and to use the original calibration set instead. \n",
    "For instance, in this tutorial we will use YUY2 -> YUV layer without emulation because we want the emulator input and the calibration dataset to remain in YUV format. \n",
    "The format conversion layer would be relevant only when running the compiled .hef file on device.\n",
    "\n",
    "Note: The NV21 -> YUV conversion is not supported in emulation.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1) Initialize Client Runner\n",
    "2) Load YUV dataset\n",
    "3) Load model script with the relevant commands\n",
    "4) Using the optimize() API, the commands are applied and the model is quantized\n",
    "5) Usage:\n",
    "  * To create input conversion after a specific layer: yuv_to_rgb_layer = input_conversion(input_layer1, yuv_to_rgb)\n",
    "  * To include the conversion in the optimization process: yuv_to_rgb_layer = input_conversion(input_layer1, yuv_to_rgb, emulator_support=True)\n",
    "  * To create input conversion after all input layers: net_scope1/yuv2rgb1, net_scope2/yuv2rgb2 = input_conversion(yuv_to_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the original parsed model again\n",
    "model_name = \"resnet_v1_18\"\n",
    "hailo_model_har_name = f\"{model_name}_hailo_model.har\"\n",
    "assert os.path.isfile(hailo_model_har_name), \"Please provide valid path for HAR file\"\n",
    "runner = ClientRunner(har=hailo_model_har_name)\n",
    "\n",
    "# We are using a pre-made YUV calibration set\n",
    "calib_dataset_yuv = np.load(\"../model_modifications/calib_dataset_yuv.npz\")\n",
    "\n",
    "# Now we're adding yuy2_to_yuv conversion before the yuv_to_rgb and a normalization layer.\n",
    "# The order of the layers is determined by the order of the commands in the model script:\n",
    "# First we add normalization to the original input layer -> the input to the network is now normalization1\n",
    "# Then we add yuv_to_rgb layer, so the order will be: yuv_to_rgb1->normalization1->original_network\n",
    "# Lastly, we add yuy2_to_yuv layer, so the order will be: yuy2_to_yuv1->yuv_to_rgb1->normalization1->original_network\n",
    "model_script_commands = [\n",
    "    \"normalization1 = normalization([123.675, 116.28, 103.53], [58.395, 57.12, 57.375])\\n\",\n",
    "    \"yuv_to_rgb1 = input_conversion(yuv_to_rgb)\\n\",\n",
    "    \"yuy2_to_yuv1 = input_conversion(input_layer1, yuy2_to_hailo_yuv)\\n\",\n",
    "]\n",
    "runner.load_model_script(\"\".join(model_script_commands))\n",
    "\n",
    "# Notice that we don't have to call runner.optimize_full_precision(), its only an intermediate step\n",
    "# to be able to use SdkFPOptimize emulator before Optimization.\n",
    "runner.optimize(calib_dataset_yuv[\"yuv_dataset\"])\n",
    "\n",
    "modified_model_har_name = f\"{model_name}_modified.har\"\n",
    "runner.save_har(modified_model_har_name)\n",
    "\n",
    "!hailo visualizer {hailo_model_har_name} --no-browser\n",
    "SVG(\"resnet_v1_18.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding On-chip Input Resize Through Model Script Commands\n",
    "This block will apply on-chip bilinear image resize at the beginning of the network through model script commands:\n",
    "\n",
    "* Create a bigger (640x480) calibration set out of the Imagenet dataset\n",
    "* Initialize Client Runner\n",
    "* Load the new calibration set\n",
    "* Load the model script with the resize command\n",
    "* Using the optimize() API, the command is applied and the model is quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = \"../data\"\n",
    "images_list = [img_name for img_name in os.listdir(images_path) if os.path.splitext(img_name)[1] == \".jpg\"]\n",
    "\n",
    "idx_to_visualize = None\n",
    "images_list = images_list[:64]\n",
    "calib_dataset_new = np.zeros((len(images_list), 480, 640, 3))\n",
    "for idx, img_name in enumerate(images_list):\n",
    "    img = Image.open(os.path.join(images_path, img_name))\n",
    "    resized_image = np.array(img.resize((640, 480), Image.Resampling.BILINEAR))\n",
    "    calib_dataset_new[idx, :, :, :] = resized_image\n",
    "    # find an image that will be nice to display\n",
    "    if idx_to_visualize is None and img.size[0] != 640:\n",
    "        idx_to_visualize = idx\n",
    "        img_to_show = img\n",
    "\n",
    "\n",
    "np.save(\"calib_set_480_640.npy\", calib_dataset_new)\n",
    "plt.imshow(img_to_show)\n",
    "plt.title(\"Original image\")\n",
    "plt.show()\n",
    "plt.imshow(np.array(calib_dataset_new[idx_to_visualize, :, :, :], np.uint8))\n",
    "plt.title(\"Resized image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet_v1_18\"\n",
    "hailo_model_har_name = f\"{model_name}_hailo_model.har\"\n",
    "assert os.path.isfile(hailo_model_har_name), \"Please provide valid path for HAR file\"\n",
    "runner = ClientRunner(har=hailo_model_har_name)\n",
    "\n",
    "calib_dataset_large = np.load(\"calib_set_480_640.npy\")\n",
    "\n",
    "# Add a bilinear resize from 480x640 to the network's input size - in this case, 224x224.\n",
    "# The order of the layers is determined by the order of the commands in the model script:\n",
    "# First we add normalization to the original input layer -> the input to the network is now normalization1\n",
    "# Then we add resize layer, so the order will be: resize_input1->normalization1->original_network\n",
    "model_script_commands = [\n",
    "    \"normalization1 = normalization([123.675, 116.28, 103.53], [58.395, 57.12, 57.375])\\n\",\n",
    "    \"resize_input1= resize(resize_shapes=[480,640])\\n\",\n",
    "]\n",
    "\n",
    "runner.load_model_script(\"\".join(model_script_commands))\n",
    "calib_dataset_dict = {\"resnet_v1_18/input_layer1\": calib_dataset_large}  # In our case there is only one input layer\n",
    "runner.optimize(calib_dataset_dict)\n",
    "\n",
    "modified_model_har_name = f\"{model_name}_resized.har\"\n",
    "runner.save_har(modified_model_har_name)\n",
    "\n",
    "!hailo visualizer {hailo_model_har_name} --no-browser\n",
    "SVG(\"resnet_v1_18.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Non-Maximum Suppression (NMS) Layer Through Model Script Commands\n",
    "This block will add an NMS layer at the end of the network through the model script command: `nms_postprocess`. The following arguments can be used to:\n",
    "\n",
    "* Config json: an external json file that allows the changing of the NMS parameters (can be skipped for the default configuration).\n",
    "* Meta architecture: which meta architecture to use (for example, `yolov5`, `ssd`, etc). In this example, `yolov5` will be used.\n",
    "* Engine: defines the inference device for running the nms: `nn_core`, `cpu` or `auto` (this example shows `cpu`).\n",
    "\n",
    "Usage:\n",
    "\n",
    "* Initialize Client Runner\n",
    "* Translate a YOLOv5 model\n",
    "* Load the model script with the NMS command\n",
    "* Use the `optimize_full_precision()` API to apply the command (Note that `optimize()` API can also be used)\n",
    "* Display inference result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"yolov5s\"\n",
    "onnx_path = f\"../models/{model_name}.onnx\"\n",
    "assert os.path.isfile(onnx_path), \"Please provide valid path for ONNX file\"\n",
    "\n",
    "# Initialize a new client runner\n",
    "runner = ClientRunner(hw_arch=\"hailo8\")\n",
    "# Any other hw_arch can be used as well.\n",
    "\n",
    "# Translate YOLO model from ONNX\n",
    "runner.translate_onnx_model(onnx_path, end_node_names=[\"Conv_298\", \"Conv_248\", \"Conv_198\"])\n",
    "# Note: NMS will be detected automatically, with a message that contains:\n",
    "#   - 'original layer name': {'w': [WIDTHS], 'h': [HEIGHTS], 'stride': STRIDE, 'encoded_layer': TRANSLATED_LAYER_NAME}\n",
    "# Use nms_postprocess(meta_arch=yolov5) to add the NMS.\n",
    "\n",
    "# Add model script with NMS layer at the network's output.\n",
    "model_script_commands = [\n",
    "    \"normalization1 = normalization([0.0, 0.0, 0.0], [255.0, 255.0, 255.0])\\n\",\n",
    "    \"resize_input1= resize(resize_shapes=[480,640])\\n\",\n",
    "    \"nms_postprocess(meta_arch=yolov5, engine=cpu, nms_scores_th=0.2, nms_iou_th=0.4)\\n\",\n",
    "]\n",
    "# Note: Scores threshold of 0.0 means no filtering, 1.0 means maximal filtering. IoU thresholds are opposite: 1.0 means filtering boxes only if they are equal, and 0.0 means filtering with minimal overlap.\n",
    "runner.load_model_script(\"\".join(model_script_commands))\n",
    "\n",
    "# Apply model script changes\n",
    "runner.optimize_full_precision()\n",
    "\n",
    "# Infer an image with the Hailo Emulator\n",
    "with runner.infer_context(InferenceContext.SDK_FP_OPTIMIZED) as ctx:\n",
    "    nms_output = runner.infer(ctx, calib_dataset_new[:16, ...])\n",
    "HEIGHT = 480\n",
    "WIDTH = 640\n",
    "# For each image\n",
    "for i in range(16):\n",
    "    found_any = False\n",
    "    min_score = None\n",
    "    max_score = None\n",
    "    # Go over all classes\n",
    "    for class_index in range(nms_output.shape[1]):\n",
    "        score, box = nms_output[i][class_index, 4, :], nms_output[i][class_index, 0:4, :]\n",
    "        # Go over all detections\n",
    "        for detection_idx in range(box.shape[1]):\n",
    "            cur_score = score[detection_idx]\n",
    "            # Discard null detections (because the output tensor is always padded to MAX_DETECTIONS on the emulator interface.\n",
    "            # Note: On HailoRT APIs (that are used on the Inference Tutorial, and with C++ APIs), the default is a list per class. For more information look for NMS on the HailoRT user guide.\n",
    "            if cur_score == 0:\n",
    "                continue\n",
    "\n",
    "            # Plotting code\n",
    "            if not found_any:\n",
    "                found_any = True\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.imshow(Image.fromarray(np.array(calib_dataset_new[i], np.uint8)))\n",
    "            if min_score is None or cur_score < min_score:\n",
    "                min_score = cur_score\n",
    "            if max_score is None or cur_score > max_score:\n",
    "                max_score = cur_score\n",
    "            (\n",
    "                y_min,\n",
    "                x_min,\n",
    "            ) = box[0, detection_idx] * HEIGHT, box[1, detection_idx] * WIDTH\n",
    "            y_max, x_max = box[2, detection_idx] * HEIGHT, box[3, detection_idx] * WIDTH\n",
    "            center, width, height = (x_min, y_min), x_max - x_min, y_max - y_min\n",
    "            # draw the box on the input image\n",
    "            rect = patches.Rectangle(center, width, height, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    if found_any:\n",
    "        plt.title(f\"Plot of high score boxes. Scores between {min_score:.2f} and {max_score:.2f}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Optimization - Compression and Optimization Levels\n",
    "\n",
    "For aggressive quantization (compress significant amount of weights to 4-bits), a higher optimization level will be needed to obtain good result.  \n",
    "\n",
    "For quick iterations it is always recommended to start with the default setting of the model optimizer (optimization_level=2, compression_level=1). However, when moving to production, it is recommended to work at the highest complexity level to achieve optimal accuracy. With regards to compression, users should increase it when the overall throughput/latency of the model is not good enough.  \n",
    "Note that increasing compression would have negligible effect on power-consumption so the motivation to work with higher compression level is mainly due to FPS considerations.\n",
    "\n",
    "\n",
    "Here the compression level is set to 4 (which means ~80% of the weights will be quantized into 4-bits) using the compression_level param in a model script and run the model optimization again. Using 4-bit weights might reduce the model's accuracy but will help to reduce the model's memory footprint. In this example, it can be seen that the reliability of some examples decreases after changing several layers to 4-bit weights, later the reliability will improve after applying higher optimization_level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls_lines = [\n",
    "    \"normalization1 = normalization([123.675, 116.28, 103.53], [58.395, 57.12, 57.375])\\n\",\n",
    "    # Batch size is 8 by default; 2 was used for stability on PCs with low amount of RAM / VRAM\n",
    "    \"model_optimization_flavor(optimization_level=0, compression_level=4, batch_size=2)\\n\",\n",
    "    # The following line is needed because resnet_v1_18 is a really small model, and the compression_level is always reverted back to 0.'\n",
    "    # To force using compression_level with small models, the following line should be used (compression level=4 equals to 80% 4-bit):\n",
    "    \"model_optimization_config(compression_params, auto_4bit_weights_ratio=0.8)\\n\",\n",
    "    # The application of the compression could be seen by the [info] messages: \"Assigning 4bit weight to layer ..\"\n",
    "]\n",
    "# -- Reduces weights memory by 80% !\n",
    "\n",
    "runner = ClientRunner(har=hailo_model_har_name)\n",
    "\n",
    "runner.load_model_script(\"\".join(alls_lines))\n",
    "runner.optimize(calib_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = calib_dataset[:IMAGES_TO_VISUALIZE, :, :, :]\n",
    "with runner.infer_context(InferenceContext.SDK_FP_OPTIMIZED) as ctx:\n",
    "    modified_res = runner.infer(ctx, images)\n",
    "with runner.infer_context(InferenceContext.SDK_QUANTIZED) as ctx:\n",
    "    quantized_res = runner.infer(ctx, images)\n",
    "\n",
    "modified_scores, modified_labels = postproc(modified_res)\n",
    "quantized_scores, quantized_labels = postproc(quantized_res)\n",
    "\n",
    "visualize_results(\n",
    "    image_dataset[:IMAGES_TO_VISUALIZE, :, :, :],\n",
    "    modified_scores,\n",
    "    modified_labels,\n",
    "    quantized_scores,\n",
    "    quantized_labels,\n",
    "    first_title=\"FP Modified\",\n",
    "    second_title=\"Quantized\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, repeating the same process with higher optimization level (For full information see the `Dataflow Compiler user guide / Model optimization` section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = calib_dataset[:IMAGES_TO_VISUALIZE, :, :, :]\n",
    "\n",
    "alls_lines = [\n",
    "    \"normalization1 = normalization([123.675, 116.28, 103.53], [58.395, 57.12, 57.375])\\n\",\n",
    "    # Batch size is 8 by default; 2 was used for stability on PCs with low amount of RAM / VRAM\n",
    "    \"model_optimization_flavor(optimization_level=2, compression_level=4, batch_size=2)\\n\",\n",
    "    # The following line is needed because resnet_v1_18 is a really small model, and the compression_level is always reverted back to 0.'\n",
    "    # To force using compression_level with small models, the following line should be used (compression level=4 equals to 80% 4-bit):\n",
    "    \"model_optimization_config(compression_params, auto_4bit_weights_ratio=0.8)\\n\",\n",
    "    # The application of the compression could be seen by the [info] messages: \"Assigning 4bit weight to layer ..\"\n",
    "]\n",
    "# -- Reduces weights memory by 80% !\n",
    "\n",
    "runner = ClientRunner(har=hailo_model_har_name)\n",
    "runner.load_model_script(\"\".join(alls_lines))\n",
    "runner.optimize(calib_dataset)\n",
    "\n",
    "with runner.infer_context(InferenceContext.SDK_FP_OPTIMIZED) as ctx:\n",
    "    modified_res = runner.infer(ctx, images)\n",
    "with runner.infer_context(InferenceContext.SDK_QUANTIZED) as ctx:\n",
    "    quantized_res = runner.infer(ctx, images)\n",
    "\n",
    "modified_scores, modified_labels = postproc(modified_res)\n",
    "quantized_scores_new, quantized_labels_new = postproc(quantized_res)\n",
    "\n",
    "visualize_results(\n",
    "    image_dataset[:IMAGES_TO_VISUALIZE, :, :, :],\n",
    "    modified_scores,\n",
    "    modified_labels,\n",
    "    quantized_scores_new,\n",
    "    quantized_labels_new,\n",
    "    first_title=\"FP Modified\",\n",
    "    second_title=\"Quantized\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Full precision predictions:                        {modified_labels}\\n\"\n",
    "    f\"Quantized predictions (with optimization_level=2): {quantized_labels_new} \"\n",
    "    f\"({sum(np.array(modified_labels) == np.array(quantized_labels_new))}/{len(modified_labels)})\\n\"\n",
    "    f\"Quantized predictions (with optimization_level=0): {quantized_labels} \"\n",
    "    f\"({sum(np.array(modified_labels) == np.array(quantized_labels))}/{len(modified_labels)})\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the optimized model to a Hailo Archive file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.save_har(quantized_model_har_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ff2d2d933521394b5518a81d3b3f196033808b856a175edf9cf3b4ada89feb9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
