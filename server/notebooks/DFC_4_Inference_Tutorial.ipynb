{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Inference Tutorial\n",
    "\n",
    "This tutorial describes the inference process.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* [HailoRT](https://hailo.ai/developer-zone/sw-downloads/) installed on the same virtual environment, or as part of the Hailo SW Suite.\n",
    "* Run this code in Jupyter notebook, see the Introduction tutorial for more details.\n",
    "* Run the [Compilation Tutorial](./DFC_3_Compilation_Tutorial.ipynb) before running this one.\n",
    "\n",
    "Note:\n",
    "This section demonstrates PyHailoRT, which is a python library for communication with Hailo devices.\n",
    "For evaluation purposes, refer to `hailortcli run2 --help` (or the alias `hailo run2 --help`).\n",
    "For more details on the HailoRT User Guide / Command Line Tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports used throughout the tutorial\n",
    "from multiprocessing import Process\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from hailo_platform import (\n",
    "    HEF,\n",
    "    ConfigureParams,\n",
    "    FormatType,\n",
    "    HailoSchedulingAlgorithm,\n",
    "    HailoStreamInterface,\n",
    "    InferVStreams,\n",
    "    InputVStreamParams,\n",
    "    InputVStreams,\n",
    "    OutputVStreamParams,\n",
    "    OutputVStreams,\n",
    "    VDevice,\n",
    ")\n",
    "\n",
    "from hailo_sdk_client import ClientRunner, InferenceContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone Hardware Deployment\n",
    "\n",
    "The standalone flow allows direct access to the HW, developing applications directly on top of Hailo\n",
    "core HW, using HailoRT.\n",
    "\n",
    "This way the Hailo hardware can be used without Tensorflow, and even without the Hailo Dataflow Compiler (after the HEF is built).\n",
    "\n",
    "A HEF is Hailoâ€™s binary format for neural networks. The HEF file contains:\n",
    "\n",
    "* Low level representation of the model\n",
    "* Target HW configuration\n",
    "* Weights\n",
    "* Metadata for HailoRT (e.g. input/output scaling)\n",
    "\n",
    "First create the desired target object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting VDevice params to disable the HailoRT service feature\n",
    "params = VDevice.create_params()\n",
    "params.scheduling_algorithm = HailoSchedulingAlgorithm.NONE\n",
    "\n",
    "# The target can be used as a context manager (\"with\" statement) to ensure it's released on time.\n",
    "# Here it's avoided for the sake of simplicity\n",
    "target = VDevice(params=params)\n",
    "\n",
    "# Loading compiled HEFs to device:\n",
    "model_name = \"resnet_v1_18\"\n",
    "hef_path = f\"{model_name}.hef\"\n",
    "hef = HEF(hef_path)\n",
    "\n",
    "# Get the \"network groups\" (connectivity groups, aka. \"different networks\") information from the .hef\n",
    "configure_params = ConfigureParams.create_from_hef(hef=hef, interface=HailoStreamInterface.PCIe)\n",
    "network_groups = target.configure(hef, configure_params)\n",
    "network_group = network_groups[0]\n",
    "network_group_params = network_group.create_params()\n",
    "\n",
    "# Create input and output virtual streams params\n",
    "# Quantized argument signifies whether or not the incoming data is already quantized.\n",
    "# Data is quantized by HailoRT if and only if quantized == False .\n",
    "input_vstreams_params = InputVStreamParams.make(network_group, quantized=False, format_type=FormatType.FLOAT32)\n",
    "output_vstreams_params = OutputVStreamParams.make(network_group, quantized=True, format_type=FormatType.UINT8)\n",
    "\n",
    "# Define dataset params\n",
    "input_vstream_info = hef.get_input_vstream_infos()[0]\n",
    "output_vstream_info = hef.get_output_vstream_infos()[0]\n",
    "image_height, image_width, channels = input_vstream_info.shape\n",
    "num_of_images = 10\n",
    "low, high = 2, 20\n",
    "\n",
    "# Generate random dataset\n",
    "dataset = np.random.randint(low, high, (num_of_images, image_height, image_width, channels)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Hardware Inference\n",
    "Infer the model and then display the output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {input_vstream_info.name: dataset}\n",
    "\n",
    "with InferVStreams(network_group, input_vstreams_params, output_vstreams_params) as infer_pipeline:\n",
    "    with network_group.activate(network_group_params):\n",
    "        infer_results = infer_pipeline.infer(input_data)\n",
    "        # The result output tensor is infer_results[output_vstream_info.name]\n",
    "        print(f\"Stream output shape is {infer_results[output_vstream_info.name].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Inference\n",
    "\n",
    "This section shows how to run streaming inference using multiple processes in Python.\n",
    "\n",
    "Infer will not be used and instead a send and receive model will be employed.\n",
    "The send function and the receive function will run in different processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the send and receive functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send(configured_network, num_frames):\n",
    "    vstreams_params = InputVStreamParams.make(configured_network)\n",
    "    with InputVStreams(configured_network, vstreams_params) as vstreams:\n",
    "        configured_network.wait_for_activation(1000)\n",
    "        vstream_to_buffer = {\n",
    "            vstream: np.ndarray([1] + list(vstream.shape), dtype=vstream.dtype) for vstream in vstreams\n",
    "        }\n",
    "        for _ in range(num_frames):\n",
    "            for vstream, buff in vstream_to_buffer.items():\n",
    "                vstream.send(buff)\n",
    "\n",
    "\n",
    "def recv(configured_network, num_frames):\n",
    "    vstreams_params = OutputVStreamParams.make(configured_network)\n",
    "    configured_network.wait_for_activation(1000)\n",
    "    with OutputVStreams(configured_network, vstreams_params) as vstreams:\n",
    "        for _ in range(num_frames):\n",
    "            for vstream in vstreams:\n",
    "                _data = vstream.recv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the amount of images to stream and processes, then recreate the target and run the processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the amount of frames to stream\n",
    "num_of_frames = 1000\n",
    "\n",
    "# Start the streaming inference\n",
    "send_process = Process(target=send, args=(network_group, num_of_frames))\n",
    "recv_process = Process(target=recv, args=(network_group, num_of_frames))\n",
    "recv_process.start()\n",
    "send_process.start()\n",
    "print(f\"Starting streaming (hef='{model_name}', num_of_frames={num_of_frames})\")\n",
    "with network_group.activate(network_group_params):\n",
    "    send_process.join()\n",
    "    recv_process.join()\n",
    "\n",
    "# Clean pcie target\n",
    "target.release()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFC Inference in Tensorflow Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This section is not yet supported on the Hailo-15, as it requires the Dataflow Compiler to be installed on the device.\n",
    "\n",
    "The ```runner.infer()``` method that was used for emulation in the model optimization tutorial can also be used for running inference on the Hailo device inside the ```infer_context``` environment. Before calling this function with hardware context, please make sure a HEF file is loaded to a runner, by one of the options: calling ```runner.compile()```, loading a complied HAR using ```runner.load_har()```, or setting the HEF attribute ```runner.hef```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the runner and load a compiled HAR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet_v1_18\"\n",
    "compiled_model_har_path = f\"{model_name}_compiled_model.har\"\n",
    "runner = ClientRunner(hw_arch=\"hailo8\", har=compiled_model_har_path)\n",
    "# For Mini PCIe modules or Hailo-8R devices, use hw_arch='hailo8r'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling ```runner.infer()``` within inference HW context to run on the Hailo device (```InferenceContext.SDK_HAILO_HW```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hef_path = f\"{model_name}.hef\"\n",
    "hef = HEF(hef_path)\n",
    "input_vstream_info = hef.get_input_vstream_infos()[0]\n",
    "image_height, image_width, channels = input_vstream_info.shape\n",
    "num_of_images = 10\n",
    "low, high = 2, 20\n",
    "\n",
    "with runner.infer_context(InferenceContext.SDK_HAILO_HW) as hw_ctx:\n",
    "    # Running hardware inference:\n",
    "    for i in range(10):\n",
    "        dataset = np.random.randint(low, high, (num_of_images, image_height, image_width, channels)).astype(np.uint8)\n",
    "        results = runner.infer(hw_ctx, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiler with Runtime Data\n",
    "\n",
    "This will demonstrate the usage of the HTML profiler with runtime data:\n",
    "\n",
    "Note: On the Hailo-15 device:\n",
    "\n",
    "1. The `hailortcli run2` command should be run on the device itself\n",
    "2. The created json file should be copied to the Dataflow Compiler environment\n",
    "3. The `hailo profiler` command should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet_v1_18\"\n",
    "hef_path = f\"{model_name}.hef\"\n",
    "compiled_har_path = f\"{model_name}_compiled_model.har\"\n",
    "runtime_data_path = f\"runtime_data_{model_name}.json\"\n",
    "\n",
    "# Run hailortcli (can use `hailo` instead) to run the .hef on the device, and save runtime statistics to runtime_data.json\n",
    "!hailortcli run2 -m raw measure-fw-actions --output-path {runtime_data_path} set-net {hef_path}\n",
    "!hailo profiler {compiled_har_path} --runtime-data {runtime_data_path} --out-path runtime_profiler.html\n",
    "\n",
    "\n",
    "# Instead, this command could be used: hailo profiler {compiled_har_path} --collect-runtime-data --out-path runtime_profiler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the Profiler with runtime data\n",
    "resnet_v1_18 is a small network, which fits in a single device without context-switch (it is called \"single context\"). Its FPS and Latency are always displayed.\n",
    "\n",
    "The ``--runtime-data`` flag is useful with big models, where the FPS and latency cannot be calculated on compile time. With runtime data, the profiler displays the load, config and runtime of the contexts, the fps and latency for multiple batch sizes.\n",
    "\n",
    "The runtime FPS is also displayed on the hailortcli output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
