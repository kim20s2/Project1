{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training Tutorial\n",
    "\n",
    "This tutorial is intended for advanced users, If the previous accuracy results were satisfactory, it can be omitted..\n",
    "\n",
    "This section will describe the steps for performing Quantization Aware Training (QAT) using Hailo's quantized model. It is assumed that the User already has a background in training deep neural networks.\n",
    "\n",
    "Quantization aware training - refers to a set of algorithms that incorporate full network training in a quantized domain. The technique utilizes the straight-through estimator (STE) concept to allow for backpropagation through non-differentiable operations, such as rounding and clipping, during the training process. In deep learning literature, QAT typically refers to an extended training procedure using the full dataset, labels, and multiple GPUs, similar to the original training process. However, it can also be applied in other scenarios.\n",
    "\n",
    "The main differences between the quantization-aware training method and the optimization method shown in previous tutorials are:\n",
    "\n",
    "* QAT enables training using labeled data, whereas the FineTune algorithm ([Model Optimization Tutorial](./DFC_2_Model_Optimization_Tutorial.ipynb)) is limited to training using knowledge distillation from the full precision model.\n",
    "* QAT supports running on multiple GPUs for faster training.\n",
    "* QAT allows for the use of a pipeline of networks or the integration of post-processing functions into the training procedure.\n",
    "\n",
    "In summary, QAT is a useful tool for training quantized models with labeled data and supports multi-GPU training and integration of post-processing functions. Currently, Hailo QAT only supports Keras.\n",
    "\n",
    "The remainder of this tutorial will cover the following steps:\n",
    "\n",
    "* Input definitions: In this step, we will prepare the dataset and model for training and testing.\n",
    "* Full precision training: A short training procedure will be run to initialize the model's weights.\n",
    "   * In real scenarios, a complete full precision training procedure should take place here. In this notebook, the full precision training has been shortened to simplify the tutorial.\n",
    "* Translation of the model: The model will be exported to TFlite, parsed, optimized, and evaluated using the Hailo toolchain.\n",
    "* Running QAT: Finally, quantization-aware training will be performed on the quantized model to optimize its accuracy.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* Run this code in Jupyter notebook, see the Introduction tutorial for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from hailo_sdk_client import ClientRunner, InferenceContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Definitions\n",
    "The input definitions step of this tutorial involves using the [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist) and a simple Convolutional Neural Network (CNN). The code provided will download the dataset and prepare it for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Prepare the dataset\n",
    "x_train = x_train.astype(np.float32) / 255\n",
    "x_test = x_test.astype(np.float32) / 255\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(f\"Total number of training samples: {x_train.shape[0]}\")\n",
    "print(f\"Total number of testing samples: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Precision Training\n",
    "In this step, a short training procedure will be run to initialize the model's weights. Only 5,000 images from the full training dataset will be used. The accuracy of the model will be measured on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run short training (using only 5k images)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train[:5000], y_train[:5000], batch_size=128, epochs=1)\n",
    "\n",
    "# Evaluate the results\n",
    "score = model.evaluate(x_train, y_train)\n",
    "print(f\"Train accuracy: {100 * score[1]:.3f} (Top-1)\")\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {100 * score[1]:.3f} (Top-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation of the Model\n",
    "In this step, a trained model will be exported into TFlite format to prepare it for use in the Hailo toolchain. After being translated into TFlite, the model can be parsed, optimized, and inferred using the Hailo DFC. The results of the full precision model will be compared to those of the quantized model. It is important to note that the results of the full precision model should be identical to those obtained from the Keras evaluation, while the quantized model may experience some degradation due to quantization noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to TFlite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_path = \"model.tflite\"\n",
    "with tf.io.gfile.GFile(tflite_model_path, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the TFlite model\n",
    "runner = ClientRunner(hw_arch=\"hailo8\")\n",
    "runner.translate_tf_model(tflite_model_path)\n",
    "\n",
    "# Optimize the model: enforce 60% 4-bit weights without optimization\n",
    "model_script_commands = [\n",
    "    \"model_optimization_config(compression_params, auto_4bit_weights_ratio=0.6)\\n\"\n",
    "    \"model_optimization_flavor(optimization_level=0)\\n\",\n",
    "]\n",
    "\n",
    "runner.load_model_script(\"\".join(model_script_commands))\n",
    "runner.optimize(x_train[:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the results\n",
    "with runner.infer_context(InferenceContext.SDK_QUANTIZED) as q_ctx:\n",
    "    with runner.infer_context(InferenceContext.SDK_FP_OPTIMIZED) as fp_ctx:\n",
    "        y_infer_fp = runner.infer(fp_ctx, x_test)\n",
    "        y_infer_q = runner.infer(q_ctx, x_test)\n",
    "\n",
    "# Hailo Keras model is exported with rank4 layers, expands dimensions for the y_test to match the model output shape\n",
    "y_test = np.expand_dims(y_test, axis=[1, 2])\n",
    "full_precision_result = np.count_nonzero(np.argmax(y_infer_fp, axis=-1) == np.argmax(y_test, axis=-1)) / len(y_test)\n",
    "quantize_result = np.count_nonzero(np.argmax(y_infer_q, axis=-1) == np.argmax(y_test, axis=-1)) / len(y_test)\n",
    "print(f\"Test accuracy (floating point): {100 * full_precision_result:.3f} (Top-1)\")\n",
    "print(f\"Test accuracy (quantized): {100 * quantize_result:.3f}%(Top-1)\")\n",
    "print(f\"Degradation: {100 * (full_precision_result - quantize_result):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running QAT\n",
    "In this final step, a quantized model will be optimized to enhance its accuracy. The `runner.get_keras_model` API will be used to obtain a Keras model initialized with the quantized weights. The model can then be trained using straight-through estimator (STE) method.\n",
    "\n",
    "* The `tf.distribute.MirroredStrategy` API is being used to enable synchronous training across multiple GPUs on the same machine.\n",
    "* The `runner.get_keras_model` API must be used with `trainable=True` to allow training (usage of `fit`).\n",
    "* To the Keras model additional layers, post-processing or other models can be added. For example, here a new `tf.keras.layers.Softmax` layer is being added.\n",
    "* For training, use the `fit` API provided by Keras. Training can be done with customized loss functions and different optimizers.\n",
    "* After training is complete, update the `ClientRunner` weights with the updated model. This is done using the `runner.set_keras_model` API. Only allowed changes to the Keras model includes weight changes. Once the new weights are updated, compile the model with the new weights using the `runner.compile` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.distribute.MultiWorkerMirroredStrategy().scope():\n",
    "    with runner.infer_context(InferenceContext.SDK_QUANTIZED) as ctx:\n",
    "        # Hailo Keras model is exported with rank4 layers, expands dimensions for the y_train to match the model output shape\n",
    "        y_train = np.expand_dims(y_train, axis=[1, 2])\n",
    "\n",
    "        # move numpy data to tf.data.Dataset to be used by multiple GPUs\n",
    "        train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        train_data = train_data.batch(128)\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "        train_data = train_data.with_options(options)\n",
    "\n",
    "        # get the Hailo Keras model for training\n",
    "        model = runner.get_keras_model(ctx, trainable=True)\n",
    "        model.build(train_data)\n",
    "        inputs = keras.Input(input_shape)\n",
    "        x = model(inputs)\n",
    "        outputs = keras.layers.Softmax(axis=0)(x)\n",
    "        new_model = keras.Model(inputs, outputs)\n",
    "\n",
    "        # adding external loss.\n",
    "        # note that this compile API only compiles the Keras model but doesn't compile the model to the Hailo HW.\n",
    "        new_model.build(train_data)\n",
    "        new_model.compile(\n",
    "            loss=keras.losses.CategoricalCrossentropy(),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=1e-6),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        # start QAT\n",
    "        log = new_model.fit(train_data, batch_size=128, epochs=10)\n",
    "\n",
    "        # set the Keras model after training. The model is already optimized, so do not run optimize() again.\n",
    "        runner.set_keras_model(model)\n",
    "\n",
    "# plot training curve\n",
    "plt.plot(log.history[\"accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Top-1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the results\n",
    "with runner.infer_context(InferenceContext.SDK_QUANTIZED) as q_ctx:\n",
    "    y_infer_qat = runner.infer(q_ctx, x_test)\n",
    "\n",
    "qat_result = np.count_nonzero(np.argmax(y_infer_qat, axis=-1) == np.argmax(y_test, axis=-1)) / len(y_test)\n",
    "print(f\"Test accuracy (quantized) before QAT: {100 * quantize_result:.3f} (Top-1)\")\n",
    "print(f\"Test accuracy (quantized) after QAT: {100 * qat_result:.3f} (Top-1)\")\n",
    "print(f\"Accuracy improvement: {100 * (qat_result - quantize_result):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Distillation and QAT\n",
    "QAT can gain additional accuracy with training using a teacher (the full precision model) to train the student model (the quantized model) - [knowledge distillation](https://arxiv.org/abs/1503.02531). To use the full precision model, call the `runner.get_keras_model` API with a different context and change the loss accordingly. In the following code, a new class `Distiller` is generated to distill the full precision and combine with the supervision of the labels.\n",
    "\n",
    "* Note that, Hailo's FineTune algorithm works in the same way as well (more information can be found in the DFC user guide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self._teacher = teacher\n",
    "        self._student = student\n",
    "\n",
    "    def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha=0.1, temperature=3):\n",
    "        self._teacher.model.compile()\n",
    "        self._student.model.compile()\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self._student_loss_fn = student_loss_fn\n",
    "        self._distillation_loss_fn = distillation_loss_fn\n",
    "        self._alpha = alpha\n",
    "        self._temperature = temperature\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not self._teacher.model.built:\n",
    "            self._teacher.model.build(input_shape)\n",
    "        if not self._student.model.built:\n",
    "            self._student.model.build(input_shape)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # unpack data (image, label)\n",
    "        x, y = data\n",
    "\n",
    "        # forward pass of teacher\n",
    "        teacher_predictions = self._teacher.model(x, training=False)\n",
    "        trainable_vars = [v._value for v in self._student.trainable_variables]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(trainable_vars)\n",
    "            # forward pass of student\n",
    "            student_predictions = self._student.model(x, training=True)\n",
    "\n",
    "            # compute supervised loss\n",
    "            student_loss = self._student_loss_fn(y, student_predictions / self._temperature) * self._temperature\n",
    "\n",
    "            # compute distillation loss\n",
    "            distillation_loss = (\n",
    "                self._distillation_loss_fn(\n",
    "                    teacher_predictions / self._temperature,\n",
    "                    student_predictions / self._temperature,\n",
    "                )\n",
    "                * self._temperature**2\n",
    "            )\n",
    "\n",
    "            total_loss = self._alpha * student_loss + (1 - self._alpha) * distillation_loss\n",
    "\n",
    "        # compute gradients\n",
    "        gradients = tape.gradient(total_loss, trainable_vars)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        for metric in self.metrics:\n",
    "            metric.update_state(y, student_predictions)\n",
    "        results = {m.name: m.result() for m in self._metrics}\n",
    "        results.update(\n",
    "            {\"total_loss\": total_loss, \"student_loss\": student_loss, \"distillation_loss\": distillation_loss},\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the TFlite model\n",
    "runner = ClientRunner(hw_arch=\"hailo8\")\n",
    "runner.translate_tf_model(tflite_model_path)\n",
    "\n",
    "# Optimize the model: enforce 40% 4bit weights without optimization\n",
    "model_script_commands = [\n",
    "    \"model_optimization_config(compression_params, auto_4bit_weights_ratio=0.6)\\n\"\n",
    "    \"model_optimization_flavor(optimization_level=0)\\n\",\n",
    "]\n",
    "\n",
    "runner.load_model_script(\"\".join(model_script_commands))\n",
    "runner.optimize(x_train[:1024])\n",
    "\n",
    "with runner.infer_context(InferenceContext.SDK_QUANTIZED) as ctx_q:\n",
    "    with runner.infer_context(InferenceContext.SDK_FP_OPTIMIZED) as ctx_fp:\n",
    "        # get the Hailo Keras model for training\n",
    "        student = runner.get_keras_model(ctx_q, trainable=True)\n",
    "\n",
    "        # geth the full precision model for kd\n",
    "        teacher = runner.get_keras_model(ctx_fp, trainable=False)\n",
    "\n",
    "        # create the kd model\n",
    "        distiller = Distiller(student=student, teacher=teacher)\n",
    "        distiller_input_shapes = (1, *student.model.get_input_shapes()[0])\n",
    "        distiller.build(distiller_input_shapes)\n",
    "        distiller.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=1e-6),\n",
    "            metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "            student_loss_fn=keras.losses.CategoricalCrossentropy(),\n",
    "            distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "            alpha=0.5,\n",
    "            temperature=10,\n",
    "        )\n",
    "\n",
    "        # start QAT\n",
    "        log = distiller.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "\n",
    "        # set the Keras model after training\n",
    "        runner.set_keras_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the results\n",
    "with runner.infer_context(InferenceContext.SDK_QUANTIZED) as q_ctx:\n",
    "    y_infer_qat = runner.infer(q_ctx, x_test)\n",
    "\n",
    "qat_with_kd_result = np.count_nonzero(np.argmax(y_infer_qat, axis=-1) == np.argmax(y_test, axis=-1)) / len(y_test)\n",
    "print(f\"Test accuracy (quantized) with QAT: {100 * qat_result:.3f} (Top-1)\")\n",
    "print(f\"Test accuracy (quantized) with QAT and KD: {100 * qat_with_kd_result:.3f} (Top-1)\")\n",
    "print(f\"Accuracy improvement: {100 * (qat_with_kd_result - qat_result):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
