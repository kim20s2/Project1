{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Analysis Tool Tutorial\n",
    "\n",
    "This is an advanced tutorial, if the accuracy results obtained were satisfactory it can be omitted.\n",
    "Before using it, make sure that your native (pre-quantization) results are satisfying.\n",
    "For more details refer to `Debugging Accuracy` section on the Dataflow Compiler User Guide.\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial will serve as a guide for how model quantization analysis breaks down the quantization noise per layer. The tutorial is intended to guide the user in using Hailo analyze noise tool, by using it to analyze the classification model MobileNet-v3-Large-Minimalistic.\n",
    "\n",
    "The flow is mainly comprised of:\n",
    "\n",
    "* Paths definitions: Defining the paths to the model and data for analysis.\n",
    "* Preparing the model: Initial Parse and Optimize of the model.\n",
    "* Accuracy analysis: This step is the heart of the tool, and computes the quantization noise of each layer output.  \n",
    "For each layer, the layer under analysis is the **only** quantized layer, while the rest of the model is kept in full precision.  \n",
    "This highlights the quantization sensitivity of the model to the noise of that specific layer.\n",
    "* Visualizing the results: Walk through the results of the accuracy analysis and explain the different graphs and information.\n",
    "* Re-optimizing the model: After debugging the noise we repeat the optimization process to improve the results.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* Run this code in Jupyter notebook, see the Introduction tutorial for more details.\n",
    "* Verify that you've completed the Parsing tutorial and the Model Optimization tutorial or generated analysis data in another way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from hailo_sdk_client import ClientRunner\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Definitions\n",
    "* Model path: path to the model to be used in this tutorial\n",
    "* data_path: path to preprocessed .npy image files for optimization and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"v3-large-minimalistic_224_1.0_float\"\n",
    "model_path = \"../models/\" + model_name + \".tflite\"\n",
    "assert os.path.isfile(model_path), \"Please provide valid path for the model\"\n",
    "\n",
    "data_path = \"./calib_set.npy\"\n",
    "assert os.path.isfile(data_path), \"Please provide valid path for a dataset\"\n",
    "har_path = model_name + \".har\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use GPU when running the analysis tool.  \n",
    "If there isn't one in the machine, the code will be executed on the CPU and it will take a longer time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tf.config.list_physical_devices(\"GPU\")) == 0:\n",
    "    print(\"Warning: you are running the accuracy analysis tool without a GPU, expect long running time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Model\n",
    "In this step, the model will be parsed and optimized to prepare it for analysis.\n",
    "For more details checkout the Parsing tutorial and the Model Optimization tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ClientRunner(hw_arch=\"hailo8\")\n",
    "runner.translate_tf_model(model_path, model_name)\n",
    "\n",
    "model_script = \"normalization1 = normalization([127.5, 127.5, 127.5], [127.5, 127.5, 127.5])\\n\"\n",
    "runner.load_model_script(model_script)\n",
    "\n",
    "runner.optimize(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Analysis\n",
    "Though ​most models work well with our default optimization, some suffer from high quantization noise that induces substantial accuracy degradation. As an example, we choose the MobileNet-v3-Large-Minimalistic neural network model that, due to its structural characteristics, results in a high degradation of 6% for Top-1 accuracy on the ImageNet-1K validation dataset.\n",
    "\n",
    "To analyze the source of degradation, the Hailo `analyze_noise` API will be used. The analysis tool uses a given dataset to measure the noise level in each layer and allows to pinpoint problematic layers that should be handled. The analysis tool uses the entire dataset by default, use the `data_count` argument to limit the number of images.  \n",
    "It is recommended to use at least 64 images, preferably not from the same calibration set, however, to keep the tool’s processing time to a reasonable level, it is also recommended not to use more than 100-200 images.\n",
    "\n",
    "The following is equivalent to running the CLI command:\n",
    "\n",
    "`hailo analyze-noise quantized_model_har_path --data-path data_path --batch-size 2 --data-count 16`\n",
    "\n",
    "The output is saved inside the HAR, to be visualized later on by the Profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.analyze_noise(data_path, batch_size=2, data_count=16)  # Batch size is 1 by default\n",
    "runner.save_har(har_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Results\n",
    "In this section, a general explanation for the noise analysis report will be provided.  \n",
    "\n",
    "To visualize the accuracy analysis results and debug the quantization noise, the Hailo Model Profiler will be used.  \n",
    "The Hailo Model Profiler will generate an HTML report with all the information for the model.  \n",
    "In the Optimization Details tab of the report, all the relevant information for this tutorial can be found:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hailo profiler {har_path}\n",
    "# Note: When working on a remote computer, manual opening of the HTML file may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SNR Chart\n",
    "Displayed on the top ribbon, only if the profiled HAR contains the analyze-noise data.\n",
    "\n",
    "This chart shows the sensitivity of each layer to quantization (measured separately for each output layer). To measure the quantization noise of each layer's output, iterate over all layers when the given layer is the **only** quantized layer, while the rest are kept in full precision and measure the SNR at each output layer. The number of SNR values will be the number of outputs layer affected by the quantized layer.\n",
    "The graph shows the SNR values in decibels (dB) and any value higher than 10 should be fine (higher is better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case an output layer is sensitive (low SNR) across many layers it is recommended to re-quantize with one of the following model script commands (not in the scope of this tutorial):\n",
    "\n",
    "  * Configure the output layer to 16-bit output. For example, using the model script command: `quantization_param(output_layer1, precision_mode=a16_w16)`.\n",
    "  * When possible, offload output activation to the accelerator. For example, the following command adds sigmoid activation to the output layer conv51: `change_output_activation(conv51, sigmoid)` and should be used to offload sigmoid from post-processing code to the accelerator.\n",
    "  * Use massive fine tune which is enabled by default in optimization_level=2 but can be customized. For example, specific fine-tune command: `post_quantization_optimization(finetune, policy=enabled, learning_rate=0.0001, epochs=8, batch_size=4, dataset_size=4000)`. Other useful attributes to this command are: loss_layer_names, loss_factors and loss_types which allows the user to manually edit the loss function of the fine tune training. In a case where the fine tune failed due to GPU memory, try to use a lower batch_size.\n",
    "  * Increase the optimization level. For example, `model_optimization_flavor(optimization_level=4)` will set the highest optimization level (default is 2).\n",
    "  * Decrease the compression level. For example, `model_optimization_flavor(compression_level=0)` will disable compression (default value is 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layers Information\n",
    "\n",
    "Displayed on the right when a layer is selected.\n",
    "\n",
    "This section provide per-layer detailed information that will help debug the local quantization errors in the model, for example, specific layer that is very sensitive for quantization. Note that quantization noise may stem from the layers' weights, activations or both.\n",
    "\n",
    "* **Weight Histogram**: this graph shows the weights distribution and can help to identify outliers. If outliers exist in the weight distribution, the following command can be used to clip it, for example, clip the kernel values of conv27:\n",
    "    `pre_quantization_optimization(weights_clipping, layers=[conv27], mode=percentile, clipping_values=[0.01, 99.99])`\n",
    "\n",
    "* **Activation Histogram**: this graph shows the activation distribution as collected by the layer noise analysis tool. Wide activation distribution is a major source of degradation source and in general it is strongly recommend to use a model with batch normalization after each layer to limit the layer's extreme activation values. Another important argument that affects the activation distribution is the calibration size that was used during quantization, to raise it, use the following command: `model_optimization_config(calibration, calibset_size=512)`, the default value for calibration is 64. In case of outliers in the layers' activation distribution, we recommend using the activation clipping command, for example:\n",
    "  `pre_quantization_optimization(activation_clipping, layers={*}, mode=percentile, clipping_values=[0.01, 99.99])`\n",
    "\n",
    "* **Scatter Plot**: this graph shows a comparison between full precision and quantized values of the layers' activation. The X-axis of each point in this graph is its value in full precision and Y-axis is the value after quantization. Zero quantization noise means the slope would be exactly one. In case of bias noise you expect to find many points above/below the line that represent imperfect quantization, if this is the case, you should use the following commands:\n",
    "  `post_quantization_optimization(bias_correction, policy=enabled)` and\n",
    "  `post_quantization_optimization(finetune, policy=disabled)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine these results, first plot the SNR graph for this specific model. Note that in general the profiler report should be used but here an alternative visualization will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snr_results():\n",
    "    # SNR results are saved in the params statistics object\n",
    "    params_statistics = runner.get_params_statistics()\n",
    "    out_layer = \"v3-large-minimalistic_224_1_0_float/output_layer1\"\n",
    "    layers = []\n",
    "    snr = []\n",
    "    for layer in runner.get_hn_model():\n",
    "        # We get the SNR for each analyzed layer for a specific output layer (there is only one in this case)\n",
    "        layer_snr = params_statistics.get(f\"{layer.name}/layer_noise_analysis/noise_results/{out_layer}\")\n",
    "        if layer_snr is not None:\n",
    "            layers.append(layer.name_without_scope)\n",
    "            snr.append(layer_snr[0].tolist())\n",
    "    return layers, snr\n",
    "\n",
    "\n",
    "def get_worst_snr_layers(layers, snr):\n",
    "    worst_snr_layers = [(layers[i], snr[i]) for i in np.argpartition(snr, 3)[:3]]\n",
    "    print(f\"Worst SNR is obtained in the following layers:\\n{worst_snr_layers}\")\n",
    "    return worst_snr_layers\n",
    "\n",
    "\n",
    "def plot_snr_graph(layers, snr):\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    plt.plot(layers, snr)\n",
    "    plt.title(f\"Per-Layer Logits SNR ({model_name}), higher is better.\")\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.xticks(rotation=75, fontsize=\"x-small\")\n",
    "    plt.ylabel(\"SNR\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "layers, snr = get_snr_results()\n",
    "get_worst_snr_layers(layers, snr)\n",
    "plot_snr_graph(layers, snr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Optimizing the Model\n",
    "Next, we will try to improve the model accuracy results by using specific model script commands. Specifically, we will use the `activation_clipping` command on the problematic layers to clip outliers from the output of the layers and `optimization_level=2`. For further information we refer the user to the full Accuracy report in the profiler HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ClientRunner(hw_arch=\"hailo8\")\n",
    "runner.translate_tf_model(model_path, model_name)\n",
    "\n",
    "model_script_commands = [\n",
    "    \"normalization1 = normalization([127.5, 127.5, 127.5], [127.5, 127.5, 127.5])\\n\",\n",
    "    \"model_optimization_config(calibration, calibset_size=128)\\n\",\n",
    "    \"pre_quantization_optimization(activation_clipping, layers=[dw1, conv2, conv3], mode=percentile, clipping_values=[0.5, 99.5])\\n\",\n",
    "    \"pre_quantization_optimization(weights_clipping, layers=[dw1], mode=percentile, clipping_values=[0.0, 99.99])\\n\",\n",
    "    \"model_optimization_flavor(optimization_level=2, compression_level=0)\\n\",\n",
    "]\n",
    "runner.load_model_script(\"\".join(model_script_commands))\n",
    "\n",
    "runner.optimize(data_path)\n",
    "\n",
    "runner.analyze_noise(data_path, batch_size=2, data_count=16)  # Batch size is 1 by default\n",
    "runner.save_har(har_path)\n",
    "\n",
    "!hailo profiler {har_path}\n",
    "# Note: When working on a remote computer, manual opening of the HTML file may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fixing the optimization process, it should be possible to reduce the model degradation to 1% (Top-1 accuracy on the ImageNet-1K validation dataset)\n",
    "which is usually the target goal for classification models.\n",
    "\n",
    "The improvement can also be seen from the new SNR graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers, snr = get_snr_results()\n",
    "get_worst_snr_layers(layers, snr)\n",
    "plot_snr_graph(layers, snr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "32642728d120b2488faed16b85d91d614cd416527bd45d9db7c28553facba783"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
