pi_serverpi_server는 라즈베리파이 5와 Hailo-8 AI 가속기를 사용하여 사용자의 데이터를 실시간으로 분석하는 RESTful API 서버입니다.카메라와 마이크 입력을 받아 AI 모델을 통해 사용자의 포즈(자세 교정)와 표정(웃음 여부)을 실시간으로 인식하며, 클라이언트의 요청에 따라 분석 결과를 전송합니다.🛠 설치 방법 (Installation)1. Hailo-8 하드웨어 및 펌웨어 설정먼저 Raspberry Pi 5 및 Hailo 설정 공식 가이드를 따라 라즈베리파이 5와 Hailo-8을 설정합니다.2. HailoRT 버전 확인위 가이드를 따르면 기본 Python 환경에 hailoRT 4.20 버전이 설치됩니다.⚠️ 중요: 이후 hailoRT 버전이 업데이트되더라도, 이 프로젝트는 4.20 버전에 맞게 테스트되었습니다. 4.20 버전으로 설치하는 것을 강력히 권장합니다.3. 가상 환경 확인hailoRT 설치 후, 프로젝트 폴더 내의 Python 가상 환경(env)이 정상적으로 실행되는지 확인합니다.🚀 사용 방법 (Usage)1. 가상 환경 활성화source env.sh
2. 서버 실행사용할 장치(device)를 지정하여 서버를 실행합니다.bash server.sh [DEVICE_NAME]
지원 장치 (DEVICE_NAME)webcam: 일반 웹캠 (640x480 @ 30fps 기준)file: 로컬 동영상 파일 재생a6700: Sony a6700 (1280x720 @ 30fps 기준)🧪 테스트 스크립트 용도서버의 개별 기능을 테스트하기 위한 셸 스크립트입니다.run_rt.sh: (Routing) 하나의 영상 소스에서 각 프로세스(fa, po, re)로 영상 데이터를 분배합니다. 다른 세 개의 스크립트(run_fa.sh, run_po.sh, run_re.sh)를 실행하기 전에 반드시 먼저 실행되어야 합니다.run_fa.sh: (Facial Analysis) 얼굴 표정 인식 프로세스를 실행하고 결과를 xml로 저장합니다.run_po.sh: (Pose Estimation) 자세 인식 프로세스를 실행하고 결과를 xml로 저장합니다.run_re.sh: (Recording) 녹화 프로세스를 실행합니다.예시# cmd 1
bash run_rt.sh [DEVICE_NAME]

# cmd 2
bash run_po.sh
[DEVICE_NAME]은 서버 실행 시와 같습니다.제공된 가상환경의 사용을 전제합니다.🏛️ 서버 아키텍처 및 설계 (Server Architecture & Design)본 서버는 제한된 하드웨어 자원 위에서 여러 AI 모델을 안정적으로 구동하는 것을 목표로 설계되었습니다.1. 설계 목표 (Design Goals)최대 성능 활용: Raspberry Pi 5의 CPU와 Hailo-8 NPU 자원을 최대한 효율적으로 사용하기 위해 Python의 GIL(Global Interpreter Lock) 제약을 피할 수 있는 멀티 프로세싱 아키텍처를 채택했습니다. 이를 통해 병렬 처리를 극대화하여 실시간 AI 추론 성능을 확보하고자 했습니다.안정성 및 독립성 확보: 각 기능(영상 분배, AI 분석, 녹화)을 독립된 프로세스로 분리하여 하나의 프로세스에 문제가 발생하더라도 다른 프로세스에 영향을 미치지 않도록 설계했습니다. 이는 시스템의 전체적인 안정성을 높여줍니다.데이터 동기화: 모든 분석 데이터는 프레임 단위의 시간(Timestamp) 정보를 포함하도록 하여, 추후 클라이언트에서 영상과 분석 결과를 정확하게 매칭할 수 있도록 동기화를 고려했습니다.2. 핵심 구조: 멀티 프로세스 아키텍처서버는 각자 명확한 역할을 가진 여러 독립적인 프로세스의 조합으로 동작합니다. 모든 프로세스는 root 경로에서 실행되어 모듈 import 경로 문제를 원천적으로 방지합니다.                  [카메라 입력]
                       |
                       V
[run_rt.sh: GStreamer RTSP 서버] --(영상 스트림)--> [run_fa.sh: 표정 분석] -> (facial.xml)
         |                      |
         |                      +--(영상 스트림)--> [run_po.sh: 자세 추정] -> (pose.xml)
         |                      |
         |                      +--(영상 스트림)--> [run_re.sh: 영상/음성 녹화] -> (video.mp4, audio.wav)
         |
         V
[server.sh: API 서버] --(제어)--> [프로세스 관리 및 최종 XML 취합]
3. 주요 컴포넌트 상세영상 분배기 (run_rt.sh):역할: GStreamer를 이용하여 카메라 입력을 받아 RTSP(Real Time Streaming Protocol) 서버를 구축합니다.동작: 단 한 번의 인코딩만으로 생성된 영상 스트림을 여러 AI 분석기 및 녹화기 프로세스에서 구독(subscribe)하여 사용할 수 있도록 합니다. 이는 CPU 자원의 중복 사용을 막는 핵심적인 역할을 합니다.AI 분석기 (run_fa.sh, run_po.sh):역할: RTSP 서버로부터 영상 스트림을 받아 Hailo-8 NPU를 활용해 AI 추론을 수행하고, 결과를 srv_tmp/xml/ 경로에 개별 xml 파일로 저장합니다.세부 모델:얼굴 표정 분석 (run_fa.sh): 얼굴의 Bounding Box를 추론하는 모델과 표정(웃음, 평상시 등)을 인식하는 모델을 순차적으로 실행합니다. Neutral(중립)을 제외한 유의미한 표정 변화를 시간과 함께 facial.xml에 기록합니다.자세 추정 (run_po.sh): 신체의 주요 관절 위치를 추정하여 몸의 틀어짐, 고개 방향, 특정 제스처 등을 판별하고 pose.xml에 기록합니다.XML 파일 크기 제어: 프레임 단위의 연속적인 데이터 기록으로 xml 파일이 비대해지는 것을 막기 위해, 특정 행동이 감지되면 2초간 동일 행동에 대한 기록을 멈추는 쓰로틀링(throttling) 로직이 적용되어 있습니다.녹화기 (run_re.sh):역할: RTSP 서버로부터 영상 스트림을 받아 /srv_tmp/mp4/video.mp4로, 마이크 입력을 받아 /srv_tmp/wav/audio.wav로 저장하는 단순 녹화 기능을 수행합니다.데이터 취합:역할: 클라이언트에서 녹화 종료 API를 호출하면 트리거됩니다.동작: 실행 중인 AI 분석기 및 녹화기 프로세스들을 모두 종료시킨 후, /srv_tmp/xml/ 경로에 생성된 facial.xml과 pose.xml을 하나로 병합하여 최종 결과물인 /srv_tmp/xml/log.xml을 생성합니다.4. 기술 선택 및 근거 (Technical Choices & Rationale)GStreamer: RTSP 서버 구축이 용이하고 다양한 카메라 하드웨어를 유연하게 지원하며, 무엇보다 Hailo 측에서 공식적으로 권장하는 미디어 프레임워크이기 때문에 채택했습니다.Shell Script 실행: 각 모듈을 독립적인 셸 스크립트로 실행시키는 방식은 Python의 import 경로 문제 해결, 개발 및 디버깅 과정에서의 명령어 단순화를 위해 선택된 실용적인 접근 방식입니다.XML 포맷: JSON과 달리 줄 바꿈, 공백 등 사소한 문법 오류에 비교적 덜 민감하여 데이터 구조의 안정성을 확보하고 파싱 오류 가능성을 최소화하기 위해 채택했습니다.Hailo NPU API 제약사항: 현재 HailoRT의 Python API는 비동기 추론 시 단일 출력(single output) 모델만 지원합니다. 얼굴 Bounding Box 추론 모델처럼 다중 출력(multi-output)을 가진 모델을 비동기 방식으로 효율적으로 처리하는 데 제약이 있어, NPU의 부담을 줄이고자 OpenCV(CPU) 대신 AI 모델을 사용하는 방식을 택하되 동기 방식으로 처리하는 현재의 구조가 설계되었습니다. (관련 Hailo 커뮤니티 토론)